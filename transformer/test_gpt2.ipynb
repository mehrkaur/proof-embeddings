{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GPT2 Logic Expressions Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Paths & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Configure device\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('DEVICE: ', DEVICE)\n",
    "\n",
    "GENERATIVE_MODEL_PATH = os.path.join('saved_models', 'generative')\n",
    "\n",
    "# Configure paths to .txt files for vocabulary (and merges) and get vocabulary size\n",
    "# NOTE: For the BERT model, we were using .txt but for GPT2 we will use .json\n",
    "VOCABULARY_PATH = os.path.join('data', 'vocabulary.json')\n",
    "VOCABULARY_SIZE = len(open(VOCABULARY_PATH, 'r').readlines())\n",
    "MERGES_PATH = os.path.join('data', 'merges.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regenerate Tokenizer for Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the tokenizer\n",
    "tokenizer = GPT2Tokenizer(vocab_file=VOCABULARY_PATH,\n",
    "                          merges_file=MERGES_PATH,\n",
    "                          errors='replace',\n",
    "                          unk_token='[UNK]',\n",
    "                          bos_token='[BOS]',\n",
    "                          eos_token='[EOS]',\n",
    "                          pad_token='[PAD]',\n",
    "                          sep_token='[SEP]',\n",
    "                          mask_token='[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Num. Parameters: 2702208\n"
     ]
    }
   ],
   "source": [
    "# Get the MLM model -- we will use the loss on CLOZE task to detect anomaly\n",
    "model = GPT2LMHeadModel.from_pretrained(GENERATIVE_MODEL_PATH, pad_token_id=tokenizer.eos_token_id)\n",
    "print('GPT2 Num. Parameters: %d' % model.num_parameters())\n",
    "# Place model on device\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(start_expression, split_token='[SEP]', max_length=150):\n",
    "    start_expression = start_expression if isinstance(start_expression, list) else [start_expression] \n",
    "    input_ids = tokenizer.encode(start_expression, return_tensors='pt')\n",
    "    greedy_output = model.generate(input_ids, max_length=max_length)\n",
    "    decoded_tokens = ''.join(tokenizer.convert_ids_to_tokens(greedy_output[0].tolist()))\n",
    "    return decoded_tokens.split(split_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(((~p∨p)∨~p)∨(~p∨p))',\n",
       " '((~p∨p))',\n",
       " '(~p∨p))',\n",
       " '(~p)',\n",
       " '(~p∨p)',\n",
       " '(~(((~p∨p∨p)p)∧(~q∨p)∧(~q)∧(~q)∧(~(~q∨p)∧(~r∨p)))∧(~q))∧(~r∨p))))∧(~q)∧(~r∨p)∧(~q)',\n",
       " '(~r∨p)',\n",
       " '(~r)))',\n",
       " '(((']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode('(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BOS]((~p∨p)∨~p)∨(~p∨p)',\n",
       " '(~p∨p)',\n",
       " '(~p∨p)',\n",
       " '(~p)',\n",
       " '(~p∨p))',\n",
       " '(~q∨p)',\n",
       " '(~q∨p)',\n",
       " '(~((~q∨p)p∧(~q)∧(~q))∧(~(~q∨~q))))))',\n",
       " '(~(~q)',\n",
       " '(~q∨p)',\n",
       " '(~q)',\n",
       " '(~r∨p))',\n",
       " '(~(~(~q∨p))',\n",
       " '(~r∨p)))',\n",
       " '(~r∨']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode('[BOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p((~p∨p)∨~p)∨(~p∨p)',\n",
       " '(~p∨p))',\n",
       " '(~p∨p)',\n",
       " '(~p∨p)',\n",
       " '(~q∨p))',\n",
       " '(~q∨p)',\n",
       " '(~q∨p)',\n",
       " '(~(~q∨p)∧(~q)∧(~q)∧(~(~q∨p∧(~q)∧(~q))))))',\n",
       " '(~q)',\n",
       " '(~r∨p∧[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]∧(~q)',\n",
       " '(~r∨p))',\n",
       " '(~r∨p))',\n",
       " '(~r']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q((~q∨q)∨(~q∨q)∧(~q∨q))∧(~q∨q))',\n",
       " '(((~q∨q)∧(~q)∧(~q∨q)∧T))∧T)',\n",
       " '((~(~q∨q)∧T∧T))∧T)',\n",
       " '((~(~((~p∨q∨~p∨q)∧(~p∨q)∧(~r∨q)∧T)∧(~p)))∧((~p)∧(~r∧(~r∨q))∧(~r∨q)))))∧']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p∨p∨(~p∨p)∨(~p∨p)',\n",
       " '(p∨p)',\n",
       " '(~p∨p))',\n",
       " '(~p∨p)',\n",
       " '(~p)',\n",
       " '(~p)',\n",
       " '(~p∨p)p)',\n",
       " '(~((~p∨p)p)∧(~q)∧(~q)∧(~~r∨p)∧[PAD][PAD][PAD][PAD][PAD][PAD][PAD]∧(~q)))',\n",
       " '(~q)',\n",
       " '(~(~r∨p)',\n",
       " '(~r)',\n",
       " '(~(~r∨p)',\n",
       " '(~r∨p))',\n",
       " '(~r)',\n",
       " '(~p)',\n",
       " '(']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode(['p', '∨', 'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T((~p∨p)∨~p)∨(~p∨p)',\n",
       " '(~p∨p)',\n",
       " '(~p∨p)',\n",
       " '(~p∨p)',\n",
       " '(~q∨p)',\n",
       " '(~q∨p)',\n",
       " '(~q∨p)',\n",
       " '(~q∨p)',\n",
       " '(~q∨p)',\n",
       " '(~((~q∨p∧(~q)))))',\n",
       " '(~p)',\n",
       " '(~∧(~q∨~q)',\n",
       " '(~q))',\n",
       " '(~q)',\n",
       " '(~p))',\n",
       " '(~p)',\n",
       " '((((~q∨~q∨p∧(~p∧(~']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode(['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F((~p∨p)∨(~p∨p)∧(~p∨p))',\n",
       " '(((~p∨p)∧(~p)∧(~p∨p)))',\n",
       " '(~q∨p))',\n",
       " '(~(~q∨p)∧T∧T)))',\n",
       " '(~(~((~q∨p)∧T∧T)∧(~q)∧(~r∨p)∧(~r∨p)))∧(~r)∧(~r∨p))∧(~r))∧(~q)∧(~q))∧(~r∨p)))))',\n",
       " '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode(['F'])  # Never saw this during training, bad performance is expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(start_expression, split_token='[SEP]', max_length=150, num_beams=10, early_stopping=True):\n",
    "    start_expression = start_expression if isinstance(start_expression, list) else [start_expression] \n",
    "    input_ids = tokenizer.encode(start_expression, return_tensors='pt')\n",
    "    beam_search_output = model.generate(input_ids, max_length=max_length, num_beams=num_beams, early_stopping=early_stopping)\n",
    "    decoded_tokens = ''.join(tokenizer.convert_ids_to_tokens(beam_search_output[0].tolist()))\n",
    "    return decoded_tokens.split(split_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(((~q∨q)∧(~q∨q))∧(~q∨q))∧T)',\n",
       " '(T∧T)',\n",
       " '(T∧T)',\n",
       " '(T∧(T∧(~q∨~q∨q))∧T)',\n",
       " '(T)',\n",
       " '(T∧T)',\n",
       " '(T∧T)',\n",
       " '(T∧T)∧T)',\n",
       " '(T)∧(T)∧T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T∧T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T∧(T)',\n",
       " '(T)',\n",
       " '(T)∧(T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search_decode('(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BOS]((~q∨q)∧(~q∨q)∧(~q∨q))∧T)',\n",
       " '(T∧T)',\n",
       " '(T∧T)',\n",
       " '(T∧(~q∨q))',\n",
       " '(T∧(~q∨q)∧T))',\n",
       " '(T∧(~q∨q)∧T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T∧[PAD][PAD][PAD][PAD][PAD][PAD][PAD]∧T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T)∧T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T∧T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '(T)',\n",
       " '((']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search_decode('[BOS]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODOs\n",
    "Look into other decoding approaches such as (i) beam search with repeating-ngram penalty, (ii) top-k sampling, and (iii) top-p nucleus sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
